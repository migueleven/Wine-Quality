{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b03495bd-3dd2-4796-99e9-e3e55afc19f3",
   "metadata": {},
   "source": [
    "# Notebook 4: Supervised Machine Learning üß†\n",
    "\n",
    "## Overview\n",
    "In this notebook, we apply supervised machine learning techniques to predict the quality of wine based on its chemical attributes. The primary goal is to train and evaluate models that can classify wine quality effectively. We will compare various models to identify the best-performing one.\n",
    "\n",
    "## Steps\n",
    "1. **Split Data:**\n",
    "   - Divide the dataset into training and testing sets.\n",
    "\n",
    "2. **Model Selection:**\n",
    "   - Train multiple supervised models, including Logistic Regression, Random Forest, SVM, etc.\n",
    "\n",
    "3. **Model Evaluation:**\n",
    "   - Use metrics like Accuracy, Precision, Recall, and F1-Score to evaluate performance.\n",
    "\n",
    "4. **Hyperparameter Tuning:**\n",
    "   - Optimize the best-performing model to achieve better results.\n",
    "\n",
    "5. **Feature Importance:**\n",
    "   - Identify which features have the most significant impact on wine quality.\n",
    "\n",
    "6. **Conclusion:**\n",
    "   - Summarize key findings and prepare for comparison with unsupervised methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a616deaa-994b-4372-8138-39532291b390",
   "metadata": {},
   "source": [
    "## 0. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77365e53-8de5-48cb-86a9-84307888004f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/cleaned_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799360d8-4160-4dee-bfcd-c262107ad62c",
   "metadata": {},
   "source": [
    "## 1. Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cda04c6d-419e-4a69-8d0f-3385d7b85556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (1087, 11)\n",
      "Test set shape: (272, 11)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Target variable and features\n",
    "X = df.drop('quality', axis=1)\n",
    "y = df['quality']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Check the shapes\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880c0ea2-244a-49aa-baeb-6cf14861e40e",
   "metadata": {},
   "source": [
    "## 2-3. Model Selection & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "13bbed1f-0e1b-4f32-b476-220c440fb45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bootcamp\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:04:43] WARNING: C:\\b\\abs_90_bwj_86a\\croot\\xgboost-split_1724073762025\\work\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000096 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1006\n",
      "[LightGBM] [Info] Number of data points in the train set: 1087, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score -4.911735\n",
      "[LightGBM] [Info] Start training from score -3.253507\n",
      "[LightGBM] [Info] Start training from score -0.857779\n",
      "[LightGBM] [Info] Start training from score -0.932054\n",
      "[LightGBM] [Info] Start training from score -2.093337\n",
      "[LightGBM] [Info] Start training from score -4.352120\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Model Performance:\n",
      "                 Model  Accuracy  F1-Score  Time (s)\n",
      "1        Random Forest  0.621324  0.601487  0.268245\n",
      "7             LightGBM  0.606618  0.595429  0.168436\n",
      "6              XGBoost  0.599265  0.583271  0.165150\n",
      "3     SVM (RBF Kernel)  0.591912  0.564441  0.061055\n",
      "5    Gradient Boosting  0.573529  0.561583  1.325994\n",
      "9          Naive Bayes  0.544118  0.554837  0.003003\n",
      "0  Logistic Regression  0.580882  0.554085  0.022019\n",
      "8  K-Nearest Neighbors  0.562500  0.547114  0.018016\n",
      "4        Decision Tree  0.507353  0.512621  0.009008\n",
      "2  SVM (Linear Kernel)  0.562500  0.506904  0.047043\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Scale the data (important for Logistic Regression and SVM)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Adjust classes to be consecutive for XGBoost\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Redefine models with adjustments\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=2000, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"SVM (Linear Kernel)\": SVC(kernel='linear', random_state=42),\n",
    "    \"SVM (RBF Kernel)\": SVC(kernel='rbf', random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42, objective='multiclass'),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB()\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    start_time = time.time()\n",
    "    if name == \"XGBoost\":  # Use encoded labels for XGBoost\n",
    "        model.fit(X_train_scaled, y_train_encoded)\n",
    "        y_pred = label_encoder.inverse_transform(model.predict(X_test_scaled))\n",
    "    else:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    # Append results\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"F1-Score\": f1,\n",
    "        \"Time (s)\": elapsed_time\n",
    "    })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results).sort_values(by=\"F1-Score\", ascending=False)\n",
    "\n",
    "# Display results\n",
    "print(\"Model Performance:\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1a1c96-beed-4ff1-8401-fdd8b07f76f4",
   "metadata": {},
   "source": [
    "## Checking Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "bd593083-207b-46a7-ab01-89cd9556072c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Performance:\n",
      "Accuracy: 1.0000\n",
      "F1-Score: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "\n",
      "Test Performance:\n",
      "Accuracy: 0.6250\n",
      "F1-Score: 0.6051\n",
      "Precision: 0.5917\n",
      "Recall: 0.6250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bootcamp\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "\n",
    "# Entrenar el modelo RandomForest por defecto\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest_default = RandomForestClassifier(random_state=42)\n",
    "random_forest_default.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones en conjunto de entrenamiento y prueba\n",
    "y_train_pred = random_forest_default.predict(X_train)\n",
    "y_test_pred = random_forest_default.predict(X_test)\n",
    "\n",
    "# M√©tricas en el conjunto de entrenamiento\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_f1 = f1_score(y_train, y_train_pred, average='weighted')\n",
    "train_precision = precision_score(y_train, y_train_pred, average='weighted')\n",
    "train_recall = recall_score(y_train, y_train_pred, average='weighted')\n",
    "\n",
    "# M√©tricas en el conjunto de prueba\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "test_precision = precision_score(y_test, y_test_pred, average='weighted')\n",
    "test_recall = recall_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "# Imprimir resultados\n",
    "print(\"\\nTraining Performance:\")\n",
    "print(f\"Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"F1-Score: {train_f1:.4f}\")\n",
    "print(f\"Precision: {train_precision:.4f}\")\n",
    "print(f\"Recall: {train_recall:.4f}\")\n",
    "\n",
    "print(\"\\nTest Performance:\")\n",
    "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"F1-Score: {test_f1:.4f}\")\n",
    "print(f\"Precision: {test_precision:.4f}\")\n",
    "print(f\"Recall: {test_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25967d29-c26e-4648-911d-7b79744dc221",
   "metadata": {},
   "source": [
    "## Check desbalanceo de clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "cf34564a-8f29-4102-8197-0111f4190486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quality\n",
       "5    577\n",
       "6    535\n",
       "7    167\n",
       "4     53\n",
       "8     17\n",
       "3     10\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b76274-2c5f-4a88-82e3-e42ec6b52f6f",
   "metadata": {},
   "source": [
    "## Oversampling with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "0441056d-69e9-45f0-9037-0b5ed8b68c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution (Train): Counter({5: 461, 6: 428, 7: 134, 4: 42, 8: 14, 3: 8})\n",
      "Original class distribution (Test): Counter({5: 116, 6: 107, 7: 33, 4: 11, 8: 3, 3: 2})\n",
      "Class distribution after SMOTE (Train): Counter({6: 461, 7: 461, 5: 461, 4: 461, 8: 461, 3: 461})\n",
      "Training Random Forest...\n",
      "Training Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bootcamp\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Support Vector Machine...\n",
      "Training K-Nearest Neighbors...\n",
      "Training Decision Tree...\n",
      "Training LightGBM...\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000164 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2805\n",
      "[LightGBM] [Info] Number of data points in the train set: 2766, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score -1.791759\n",
      "[LightGBM] [Info] Start training from score -1.791759\n",
      "[LightGBM] [Info] Start training from score -1.791759\n",
      "[LightGBM] [Info] Start training from score -1.791759\n",
      "[LightGBM] [Info] Start training from score -1.791759\n",
      "[LightGBM] [Info] Start training from score -1.791759\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "\n",
      "Model Performance Comparison:\n",
      "                    Model  Accuracy  F1-Score  Balanced Accuracy  \\\n",
      "0           Random Forest  0.551471  0.563075           0.304419   \n",
      "5                LightGBM  0.551471  0.554117           0.283220   \n",
      "1     Logistic Regression  0.419118  0.473100           0.317250   \n",
      "4           Decision Tree  0.437500  0.457992           0.267411   \n",
      "3     K-Nearest Neighbors  0.334559  0.383982           0.198075   \n",
      "2  Support Vector Machine  0.279412  0.329327           0.173272   \n",
      "\n",
      "   Time Taken (s)  \n",
      "0        0.612557  \n",
      "5        0.226488  \n",
      "1        0.527737  \n",
      "4        0.032029  \n",
      "3        0.020018  \n",
      "2        0.379345  \n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from collections import Counter\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Separar features (X) y target (y)\n",
    "X = df.drop(columns=['quality'])  # Reemplaza 'quality' con tu columna objetivo\n",
    "y = df['quality']\n",
    "\n",
    "# Dividir datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Ver distribuci√≥n original de clases\n",
    "print(\"Original class distribution (Train):\", Counter(y_train))\n",
    "print(\"Original class distribution (Test):\", Counter(y_test))\n",
    "\n",
    "# Aplicar SMOTE al conjunto de entrenamiento\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Ver nueva distribuci√≥n de clases despu√©s de SMOTE\n",
    "print(\"Class distribution after SMOTE (Train):\", Counter(y_train_smote))\n",
    "\n",
    "# Lista de modelos\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Support Vector Machine\": SVC(random_state=42),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# DataFrame para almacenar los resultados\n",
    "results = []\n",
    "\n",
    "# Probar cada modelo\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Entrenar el modelo\n",
    "    model.fit(X_train_smote, y_train_smote)\n",
    "    \n",
    "    # Predecir en el conjunto de prueba\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calcular m√©tricas\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    bal_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    # Guardar resultados\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": acc,\n",
    "        \"F1-Score\": f1,\n",
    "        \"Balanced Accuracy\": bal_acc,\n",
    "        \"Time Taken (s)\": elapsed_time\n",
    "    })\n",
    "\n",
    "# Crear un DataFrame con los resultados\n",
    "results_df = pd.DataFrame(results).sort_values(by=\"F1-Score\", ascending=False)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03316460-7f0b-40ed-b018-d23ec5d74c7d",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8a0463e8-7d87-4c84-86ab-6ec29835d8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2592 candidates, totalling 12960 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bootcamp\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 4, 'max_features': 'sqrt', 'max_leaf_nodes': 14, 'n_estimators': 30}\n",
      "Accuracy: 0.6066\n",
      "F1-Score: 0.5677\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = { \n",
    "    'n_estimators': [20,25,30,35,40,35,50,60,70,75,80,85], \n",
    "    'max_depth': [1,2,3,4,5,6],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'max_leaf_nodes': [9,10,11,12,13,14,15,16,17,18,19,20], \n",
    "} \n",
    "\n",
    "# StratifiedKFold for better cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# GridSearchCV with RandomForest\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    verbose=1,\n",
    "    n_jobs=-1  # Use multiple cores to speed up\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea16a1a0-de84-4bb0-a431-85e59e317b32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
